{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VictorPontual/ResNet_ImageClassifier/blob/main/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO1HbFIJrVZh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Essa parte serve para descomprimir o dataset. Deve-se atentar ao endereço do arquivo .zip, uma vez que pode estar diferente dependendo de onde seja alocado nos seus arquivos pessoais\n"
      ],
      "metadata": {
        "id": "HEIB2fof6eLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -n /content/drive/MyDrive/Colab_Notebooks/Nonsegmented.zip -d ./plant-seedlings-classification"
      ],
      "metadata": {
        "id": "tA85DlYUbk7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código define geradores de dados usando ImageDataGenerator para realizar aumento de dados (data augmentation) e pré-processamento de imagens armazenadas em um diretório. Ele separa os dados em conjuntos de treinamento e validação (80/20), aplicando transformações como rotação, deslocamento, cisalhamento, zoom e flips para melhorar a generalização do modelo."
      ],
      "metadata": {
        "id": "FDYTicTLzCjA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va5S73CzsD92"
      },
      "outputs": [],
      "source": [
        "def define_generators():\n",
        "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rotation_range=360,\n",
        "        width_shift_range=0.3,\n",
        "        height_shift_range=0.3,\n",
        "        shear_range=0.3,\n",
        "        zoom_range=0.5,\n",
        "        vertical_flip=True,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.2, # Separar 20% dos dados para validação\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        directory='./plant-seedlings-classification/Nonsegmented', # Caminho para a pasta principal\n",
        "        target_size=(width, height),\n",
        "        batch_size=batch_size,\n",
        "        color_mode='rgb',\n",
        "        class_mode=\"categorical\",\n",
        "        subset='training', # Usar os dados de treinamento\n",
        "    )\n",
        "\n",
        "    validation_generator = train_datagen.flow_from_directory(\n",
        "        directory='./plant-seedlings-classification/Nonsegmented', # Caminho para a pasta principal\n",
        "        target_size=(width, height),\n",
        "        batch_size=batch_size,\n",
        "        color_mode='rgb',\n",
        "        class_mode=\"categorical\",\n",
        "        subset='validation', # Usar os dados de validação\n",
        "    )\n",
        "\n",
        "    return train_generator, validation_generator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código define três callbacks para monitorar e ajustar o treinamento do modelo de forma eficiente:\n",
        "\n",
        "1. **`ModelCheckpoint`**: Salva o modelo no arquivo `model.keras` apenas quando há uma melhora na métrica de validação (`val_accuracy`), garantindo que o melhor modelo seja armazenado.\n",
        "\n",
        "2. **`EarlyStopping`**: Interrompe o treinamento antecipadamente se a perda de validação (`val_loss`) não melhorar por 10 épocas consecutivas, evitando sobreajuste e economizando tempo.\n",
        "\n",
        "3. **`ReduceLROnPlateau`**: Reduz a taxa de aprendizado em um fator de 0.5 se a perda de validação (`val_loss`) não melhorar por 3 épocas, ajudando na convergência em estágios finais.\n",
        "\n",
        "Esses callbacks são retornados como uma lista para serem usados durante o treinamento do modelo."
      ],
      "metadata": {
        "id": "YU1As5hkzr_t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88-Azzptsigu"
      },
      "outputs": [],
      "source": [
        "def define_callbacks():\n",
        "    save_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath='model.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        verbose=1\n",
        "    )\n",
        "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    return [save_callback, early_stopping, lr_scheduler]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código define hiperparâmetros e configurações básicas para o treinamento de um modelo de classificação de imagens:\n",
        "\n",
        "1. **`nb_epoch`**: Define o número total de épocas de treinamento (40).\n",
        "\n",
        "2. **`batch_size`**: Configura o tamanho do lote (16 imagens por batch).\n",
        "\n",
        "3. **`width` e `height`**: Especificam a dimensão das imagens (299x299 pixels) para redimensionamento. tamanho escolhido para dar uma maior resolução aos detalhes das imagens, algo de extrema importância no dataset avaliado.\n",
        "\n",
        "4. **`species_list`**: Lista as classes do dataset, representando diferentes espécies de plantas que o modelo será treinado para classificar.\n",
        "\n",
        "Essas definições configuram o ambiente para treinamento do modelo com um dataset de 12 classes."
      ],
      "metadata": {
        "id": "HLjoddbX0t5Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxrpVvMasn8x"
      },
      "outputs": [],
      "source": [
        "nb_epoch     = 40\n",
        "batch_size   = 16\n",
        "width        = 299\n",
        "height       = 299\n",
        "species_list = [\"Black-grass\", \"Charlock\", \"Cleavers\", \"Common Chickweed\", \"Common wheat\", \"Fat Hen\",\n",
        "                \"Loose Silky-bent\", \"Maize\", \"Scentless Mayweed\", \"Shepherds Purse\", \"Small-flowered Cranesbill\",\n",
        "                \"Sugar beet\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código implementa um **bloco residual** para redes ResNet, permitindo aprendizado eficiente em redes profundas ao usar uma **conexão de atalho** (shortcut). O bloco realiza o seguinte:\n",
        "\n",
        "1. **Entrada principal**:\n",
        "   - Aplica duas camadas convolucionais 3x3 com Batch Normalization e ativação ReLU na primeira convolução.\n",
        "   - A segunda convolução não usa ativação imediatamente para permitir a soma residual.\n",
        "\n",
        "2. **Atalho (shortcut)**:\n",
        "   - Normalmente, copia a entrada diretamente.\n",
        "   - Se `downsample=True`, ajusta o atalho usando uma convolução 1x1 com Batch Normalization para alinhar dimensões (espaciais e de canais) com a saída principal.\n",
        "\n",
        "3. **Conexão residual**:\n",
        "   - Soma o tensor do atalho (`shortcut`) com a saída das convoluções.\n",
        "   - Aplica uma ativação ReLU na soma para obter a saída final do bloco.\n",
        "\n",
        "Esse design melhora a propagação de gradientes e facilita o aprendizado em redes muito profundas."
      ],
      "metadata": {
        "id": "k5v1hntp2KDM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1s6f6IBGyuQ"
      },
      "outputs": [],
      "source": [
        "def residual_block(input_tensor, filters, stride=1, downsample=False):\n",
        "    \"\"\"\n",
        "    Define um bloco residual.\n",
        "    Args:\n",
        "        input_tensor: tensor de entrada.\n",
        "        filters: número de filtros para convoluções.\n",
        "        stride: stride das convoluções.\n",
        "        downsample: se True, aplica downsampling.\n",
        "    \"\"\"\n",
        "    shortcut = input_tensor\n",
        "\n",
        "    # Primeira camada\n",
        "    x = keras.layers.Conv2D(filters, kernel_size=(3, 3), strides=stride, padding=\"same\")(input_tensor)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.ReLU()(x)\n",
        "\n",
        "    # Segunda camada\n",
        "    x = keras.layers.Conv2D(filters, kernel_size=(3, 3), strides=1, padding=\"same\")(x)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # Ajustar o atalho (shortcut) para corresponder as dimensões\n",
        "    if downsample:\n",
        "        shortcut = keras.layers.Conv2D(filters, kernel_size=(1, 1), strides=stride)(input_tensor)\n",
        "        shortcut = keras.layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    # Somar o atalho com a saída do bloco\n",
        "    x = keras.layers.add([x, shortcut])\n",
        "    x = keras.layers.ReLU()(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código define uma função para construir uma rede neural convolucional do tipo **ResNet** personalizada, baseada em blocos residuais. O modelo é projetado para realizar tarefas de **classificação de imagens**.\n",
        "\n",
        "A função permite especificar o formato das imagens de entrada, o número de classes para a saída e a quantidade de blocos residuais em cada estágio da rede. A arquitetura é composta por uma série de camadas convolucionais, seguidas de normalização e ativação, além de camadas de pooling para redução de dimensionalidade.\n",
        "\n",
        "A rede é dividida em estágios, sendo que em cada estágio são aplicados blocos residuais que, por sua vez, ajudam a preservar informações importantes e a mitigar problemas como o desaparecimento do gradiente, comuns em redes profundas. Durante o processo, o número de filtros é duplicado a cada novo estágio, aumentando a capacidade de representação do modelo.\n",
        "\n",
        "No final, o modelo realiza um **global average pooling** para reduzir a dimensionalidade das características extraídas e, em seguida, aplica uma camada densa com ativação softmax para a classificação das imagens nas classes fornecidas. O modelo é então retornado, pronto para treinamento e avaliação."
      ],
      "metadata": {
        "id": "mQRAN6I23Hzk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WyR2v-4Gzsp"
      },
      "outputs": [],
      "source": [
        "def build_resnet(input_shape, num_classes, num_blocks=[2, 2, 2, 2]):\n",
        "    \"\"\"\n",
        "    Constrói uma ResNet personalizada.\n",
        "    Args:\n",
        "        input_shape: shape das imagens de entrada (altura, largura, canais).\n",
        "        num_classes: número de classes para a classificação.\n",
        "        num_blocks: lista indicando o número de blocos em cada estágio.\n",
        "    \"\"\"\n",
        "    inputs = keras.layers.Input(shape=input_shape)\n",
        "    x = keras.layers.Conv2D(64, kernel_size=(7, 7), strides=2, padding=\"same\")(inputs)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.ReLU()(x)\n",
        "    x = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding=\"same\")(x)\n",
        "\n",
        "    # Estágios da ResNet\n",
        "    filters = 64\n",
        "    for i, blocks in enumerate(num_blocks):\n",
        "        for j in range(blocks):\n",
        "            if j == 0 and i != 0:  # Downsampling para o primeiro bloco de cada estágio (exceto o primeiro estágio)\n",
        "                x = residual_block(x, filters, stride=2, downsample=True)\n",
        "            else:\n",
        "                x = residual_block(x, filters)\n",
        "        filters *= 2  # Dobrar os filtros a cada estágio\n",
        "\n",
        "    # Camadas finais\n",
        "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    # Criar o modelo\n",
        "    model = keras.models.Model(inputs, outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerar os dados\n",
        "train_generator, validation_generator = define_generators()\n",
        "\n",
        "# Construir o modelo (Ex.: ResNet)\n",
        "model = build_resnet(input_shape=(width, height, 3), num_classes=len(species_list))"
      ],
      "metadata": {
        "id": "og8tU6O4jWEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizar o formato dos dados\n",
        "print(len(train_generator))\n",
        "print(len(validation_generator))"
      ],
      "metadata": {
        "id": "nq-AVrM2jbL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPIuwPU9HVqK"
      },
      "outputs": [],
      "source": [
        "# Obter os callbacks\n",
        "callbacks = define_callbacks()\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=nb_epoch,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(validation_generator)\n",
        "\n",
        "predicted_classes = predictions.argmax(axis=-1)"
      ],
      "metadata": {
        "id": "HFaSoWDyj8Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Carregar um lote de 20 imagens do validation_generator\n",
        "images, true_labels = next(validation_generator)  # Gera um lote do generator\n",
        "predicted_probabilities = model.predict(images)  # Predições\n",
        "predicted_labels = np.argmax(predicted_probabilities, axis=1)  # Classes preditas\n",
        "\n",
        "# Obter nomes das classes (rótulos) do generator\n",
        "class_names = list(validation_generator.class_indices.keys())\n",
        "\n",
        "# Determine the number of images in the batch\n",
        "num_images = images.shape[0]  # Get the size of the first dimension\n",
        "\n",
        "# Plotar as imagens com rótulos reais e preditos\n",
        "plt.figure(figsize=(20, 10))\n",
        "# Iterate up to the number of images available\n",
        "for i in range(num_images):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    # Normalize image data to the range [0, 1]\n",
        "    image = images[i] / 255.0  # Assuming your images are in the range [0, 255]\n",
        "    plt.imshow(image)\n",
        "    # Convert true_labels to integer labels before indexing into class_names\n",
        "    true_class = class_names[np.argmax(true_labels[i])] # Assuming true_labels is one-hot encoded\n",
        "    predicted_class = class_names[predicted_labels[i]]\n",
        "    plt.title(f\"Real: {true_class}\\nPred: {predicted_class}\", color=\"green\" if true_class == predicted_class else \"red\")\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wt85hC1Ulcif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Exemplo de rótulos reais e predições (substitua pelos seus dados)\n",
        "true_labels = validation_generator.classes  # Rótulos reais\n",
        "predicted_probabilities = model.predict(validation_generator)  # Predições do modelo\n",
        "predicted_labels = np.argmax(predicted_probabilities, axis=1)  # Classes preditas\n",
        "\n",
        "# Gerar a matriz de confusão\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Exibir a matriz de confusão\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=validation_generator.class_indices.keys())\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Matriz de Confusão\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XA3Q-Fs8k7TM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1v2HuXJejg4skRCs1D4z72gR7kQJCJw5C",
      "authorship_tag": "ABX9TyN4E4Wkc5VkD1enjL+xsqxe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}